### THE WHAT
As a DevOps/SRE engineer, I spent about 20-25% of my time analyzing the causes of the slow SQL queries.
Which is the direct result of people using ORM without giving a second thought about what SQL it produces
and skipping any serious design, optimization and stress testing to get the actual numbers before it went to production.
Most of such ORM generated queries had `field in (long list of values)` in its WHERE clause.

So, I've been looking for a very simple that would do 2 things:seeds the DB tables and generats stress test SQLs from the seeds. 
And of course collects the stats during both steps.

So, this tool does the following:
- seeds the DB tables selected fields with randomly generated integer and text data per configuration: 
min and max integers and min and max length text;
- records and reports the timing of the inserts done by the configured number of threads;
- generates SQLs for stress testing that may use those randomly generated data. 
I was particularly interested in the SQLs with clauses like WHERE field IN (value1, value2, ..., value10000), 
where the values are from the set randomly generated at the prior step. I've seen way too many of those peg the DB
- This is where the seeding step ends. The next step - stress testing
- Stressing: run the generated SQLs in the configured number of threads and record the timings: max, min and the timing histogram with 100ms granularity

This is it! As I said, very simple

### Configuration

The config json file that u is supplied via the --config flag. I will explain its structure with an example that I used for testing.
So, I created 2 tables:
```bash
create table table_1(a int, b varchar(2048), c text);
create table table_2(a int, b varchar(8192), c text);
```

The config json file  has the following structure
```bash
{
  "seed" : [      // seed configuration: array of per table seed configs
    {
      "table": "table_1",   // table name
      "records": 1000000,   // how many records in total we want to generate and insert into table_q
      "insertThreads": 12,  // how many threads should run the inserts of those 1000000 records. Careful: 40 threads almost blew up my Mac
      "fields":             // array of table_1 fields' configs
      [
        {"id": "id_1",          // ID
          "field": "a",         // field name, from the table structure obviously
          "field_type": "int",  // type. Valid values int or string. Obviously should match the table def.
          "encoding": "utf-8",  // not yet used and is meaningless for the int fields
          "min": 10,            // min  of the integer interval of the generated values
          "max": 35000,         // max for the same
          "cardinality": 10000` // what it says: max number of unique values generated for this field. 
                                // alternatively, you could have specified "unique":true. This would insure that all the generated values
                                // are unique. Useful if u have a UNIQUE index on that field.
        },
        {
          "id": "id_2",         // everything has the same meaning
          "field": "b",
          "field_type": "string",
          "encoding": "utf-8",
          "min": 10,            // except this sets the minimum length of the generated text string instead of the value as it was for int
          "max": 2048,          // max length of the generated string
          "cardinality": 1000
        }
      ]
    },
    {
      "table": "table_2",       // seed config for another table
      "records": 1100000,
      "insertThreads": 10,
      "fields":
      [
        {"id": "id_1",
          "field": "a",
          "field_type": "int",
          "encoding": "utf-8",
          "min": 10,
          "max": 100000,
          "cardinality": 10000
        },
        {
          "id": "id_2",
          "field": "b",
          "field_type": "string",
          "encoding": "utf-8",
          "min": 10,
          "max": 8000,
          "cardinality": 1000
        }
      ]
    }
  ],
  "stressConfig": {
    "sqls_to_file": "./sqls-001.txt", // the file where the SQLs for stress testing will be stored
    "sql":
    [       // an array of SQL statements to run for stress testing
      {
        "id": "statement one",
        "statement": "SELECT a,b FROM table_1 WHERE a in ( {\"table\":\"table_1\", \"field\":\"a\", \"minlen\": 30, \"maxlen\": 100}) AND b in({\"table\":\"table_1\", \"field\":\"b\", \"minlen\": 10, \"maxlen\": 40})",
        // this is the template for generating SQLs for stress testing    
        // the interesting part is {\"table\":\"table_1\", \"field\":\"a\", \"minlen\": 30, \"maxlen\": 100}
        // this json defines how the IN list will be built out of the values generated by the seed step
        // table and field have obvious meaning
        // minlen and maxlen set the boundaries for the random length of the list
        // You would only have this JSON definition if you have an IN list in your SQL statement. It does not have to be the case
        // see the SQL JOIN statement below
        // in the future we should probably provide an option to specify a percentage of values that are not in the table
        "repeat" : 10000, // how many SQL statements to generate. Note: if that IN list JSON config is present, they will all be different
                          // since the list of random length between minlen and maxlen will be randomly generated.
                          // if that json is not present in the SQL template, all generated SQL statements will be the same
        "threads": 10,    // how many threads to use for stress testing with these "repeat" SQLs.
        "comment": "the in list {escaped json} will be randomly populated from the seeded data"
                          // this will create the following records in the sqls_to_file file:
                          // ID = statement+one
                          // THREADS = 10
                          // SELECT a,b FROM table_1 WHERE a in ( 10746, 13147, 16020, 5929, 15020, 3105, ....
                          // SELECT a,b ......
                          // and so on, up to the "repeat" value
      },
      {
        "id": "statement two",
        "statement": "SELECT a,b FROM table_2 WHERE a in ( {\"table\":\"table_1\", \"field\":\"a\", \"minlen\": 10, \"maxlen\": 30}) AND b in({\"table\":\"table_2\", \"field\":\"b\", \"minlen\": 10, \"maxlen\": 90})",
        "repeat" : 10000,
        "threads": 8,
        "comment": "the in list {escaped json} will be randomly populated from the seeded data"
      },
      {
        "id": "join",
        "statement": "SELECT count(1) FROM table_1 x JOIN table_2 y ON x.a=y.a WHERE x.a in ( {\"table\":\"table_1\", \"field\":\"a\", \"minlen\": 10, \"maxlen\": 20}) AND x.b in({\"table\":\"table_1\", \"field\":\"b\", \"minlen\": 5, \"maxlen\": 30})",
        "repeat" : 100,
        "threads": 10,
        "comment": "the in list {escaped json} will be randomly populated from the seeded data"
      }
    ]
  }
}

```
# Running and Command Flags

You run it with `seed` command to seed the DB and produce the SQLs file. 
And then run it with `stress` to stress test your DB with the seeded data and generated SQLs.
Take a look at the docker-compose.yml and ./cmd/cli.go. They are pretty self explanatory. 
The --out-dir/OUT_DIR flag/env var specifies where the files with the timing results will be placed. 
There will be 2 files placed there: 
- durations.txt - stores the duration of each SQL - INSERT if it's the seed phase or 
SELECT (or INSERT/UPDATE/DELETE if you chose to stress test with those) if it's the stress phase.
Here is how it looks for the stress phase of my test:
```bash
ID=statement+one Thread=thread-7 Duration=194.11265ms SQL=SELECT a,b FROM table_1 WHERE a in ( 11510, 27606, 11447, 20249, 1373, 20945, 20318, 24378, 4272, ...
ID=statement+one Thread=thread-2 Duration=190.890384ms SQL=SELECT a,b FROM table_1 WHERE a in ( 12760, 30652, 29791, 322, 31350, 22263, 5182, 32679
```
- stats.txt. This file contains the aggregated by statement stats. For my stress test it looks like
```bash
- **********************************************************************
                statement+one stats
**********************************************************************
Count 10000
Total duration 2m0.662294683s
Longest sql 486.207Âµs SELECT a,b FROM table_1 WHERE a in ( 17935, 22290, 29259, 3237, 14266, 20330, 16642, ...
Shortest sql 391.734274ms SELECT a,b FROM table_1 WHERE a in ( 6226, 17546, 14310, 17423, 22688, 5080, 1038, 5155, 
under	100 ms	200 ms	300 ms	400 ms	500 ms	600 ms	700 ms	800 ms	900 ms	1000 ms	1100 ms	1200 ms	1300 ms	1400 ms	1500 ms	1600
    	9716	182	78	24	0	0	0	0	0	0	0	0	0	0
```

So, totals, longest, shortest and the histogram of timings with 100ms granularity. Same is output to stdout.
I checked the sample files into ./test/assets directory

# Supported Databases
I put in the implementation for Postgres and MySQL. ./db/db_pg.go and ./db/db_mysql.go
All u need to do to extend it to others is to implement this interface
```bash
type database interface {
	connect(cc *cli.Context) error
	close(cc *cli.Context) error
	buildInsert(table string, fields []string) string
	exec(cc *cli.Context, sql string, arguments []any) error
	execLiteral(cc *cli.Context, sql string) error
}
```
and put it in the package db. Pretty trivial.
I needed custom buildInsert functions cuz Pg expects parameters list as ($1,$2,..,$N) while MySQL needs (?,?,?,?).
I included the execLiteral function in instead of using some variadic parameters simply because I like it this way.
Your DB might need something else.

# Few words on the tool architecture

The seeder works table after table from the config. 
It generates the random values per spec and then spawns the specified number of go routines, each inserting its subslice of generated values.
Stats are collected by a stats go routine via a channel.

The stresser reads the the generated SQLs file record by record:
- reads in the ID
- reads in the number of threads for that ID
- spawns the "number of threads" go routines
- keeps reading the SQLs from the file and pass it to the go routines via a channel
- when it reaches another ID - which means another set of generated SQLs (new SQL template) - or the end of the file, 
it writes poison pills to the channel, go routines read them and exit.
- again, stats are collected by a stats collector go routine via a channel

# ContaineriZation

Common, people, this is a very simple go program. Write your own Dockerfile.
I have checked in ./.docker/docker-compose.yml for the e2e testing

That's all, folks!
